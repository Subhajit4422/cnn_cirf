{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Done by Subhajit**"],"metadata":{"id":"-K3luZMVMBJa"}},{"cell_type":"code","metadata":{"id":"aCBYFLVHICl4"},"source":["from tensorflow.keras import models, layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import BatchNormalization, Activation, Flatten"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UpTr48AGJDe0"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4bfAV91JGx6"},"source":["# Hyperparameters\n","batch_size = 128\n","num_classes = 10\n","epochs = 300\n","num_filter = 35\n","compression = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iv9R1EoDJMJs","outputId":"18191a34-09eb-44ac-891f-331af02e9f20","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686981094559,"user_tz":-330,"elapsed":6355,"user":{"displayName":"Subhajit Sarkar","userId":"12932209816484918809"}}},"source":["# Load CIFAR10 Data\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n","\n","# convert to one hot encoing\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 3s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"d47TRlrj5hTO"},"source":["X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","X_train /= 255\n","X_test /= 255"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvQZ3OPU53mL"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMzhtYvl5mVy"},"source":["traindataGen=ImageDataGenerator(shear_range=0.2,zoom_range=0.2, rotation_range=20, width_shift_range=0.2,\n","height_shift_range=0.2,horizontal_flip=True,fill_mode='nearest')\n","testdataGen=ImageDataGenerator()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_yH650l6N4N"},"source":["traindataGen.fit(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbffjUklJfct","outputId":"aa48606d-07ba-4d84-dbcb-8cf8b13ea0e5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686981116247,"user_tz":-330,"elapsed":12,"user":{"displayName":"Subhajit Sarkar","userId":"12932209816484918809"}}},"source":["print('X_train shape =', X_train.shape)\n","print('X_test shape =', X_test.shape)\n","print('y_train shape =', y_train.shape)\n","print('y_test shape =', y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape = (50000, 32, 32, 3)\n","X_test shape = (10000, 32, 32, 3)\n","y_train shape = (50000, 10)\n","y_test shape = (10000, 10)\n"]}]},{"cell_type":"code","metadata":{"id":"zTR1DJW5XyQZ"},"source":["from tensorflow.keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5smh6wDYwAT"},"source":["import os\n","os.mkdir('model_save')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NrXDlmTNVx4W"},"source":["import tensorflow.keras.backend as k\n","def changeLearningRate(epoch):\n","  ''' This function changes learing rate based on validation accuracy and epoch number'''\n","  learning_rate = k.eval(model.optimizer.lr)\n","  if epoch == epochs/2:\n","     #The learning rate is divided by 10 at 50% of the total number of training epochs\n","    learning_rate = learning_rate * 0.1\n","   #The learning rate is divided by 10 at 75% of the total number of training epochs\n","  elif epoch == (epochs*3)/4:\n","      learning_rate = learning_rate * 0.1\n","  return learning_rate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulLZ6t4_Jlbh"},"source":["# Dense Block\n","def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n","    global compression\n","    temp = input\n","    for _ in range(l):\n","        BatchNorm = layers.BatchNormalization()(temp)\n","        relu = layers.Activation('relu')(BatchNorm)\n","        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n","        if dropout_rate>0:\n","            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n","        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n","\n","        temp = concat\n","\n","    return temp\n","\n","## transition Blosck\n","def transition(input, num_filter = 12, dropout_rate = 0.2):\n","    global compression\n","    BatchNorm = layers.BatchNormalization()(input)\n","    relu = layers.Activation('relu')(BatchNorm)\n","    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n","    if dropout_rate>0:\n","         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n","    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n","    return avg\n","\n","#output layer\n","def output_layer(input):\n","    global compression\n","    BatchNorm = layers.BatchNormalization()(input)\n","    relu = layers.Activation('relu')(BatchNorm)\n","    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n","    flat = layers.Flatten()(AvgPooling)\n","    output = layers.Dense(num_classes, activation='softmax')(flat)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JOhzAGbJpPG"},"source":["l = 6\n","dropout_rate = 0\n","input = layers.Input(shape=(img_height, img_width, channel))\n","First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n","\n","First_Block = denseblock(First_Conv2D, num_filter, dropout_rate=0)\n","First_Transition = transition(First_Block, num_filter, dropout_rate=0)\n","\n","Second_Block = denseblock(First_Transition, num_filter, dropout_rate=0)\n","Second_Transition = transition(Second_Block, num_filter, dropout_rate=0)\n","\n","Third_Block = denseblock(Second_Transition, num_filter, dropout_rate=0)\n","Third_Transition = transition(Third_Block, num_filter, dropout_rate=0)\n","\n","Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate=0)\n","output = output_layer(Last_Block)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAeExyIlKYUv","outputId":"8bdd2ebc-4b50-42ae-fefd-8c6d05c2a6cd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686981140728,"user_tz":-330,"elapsed":1319,"user":{"displayName":"Subhajit Sarkar","userId":"12932209816484918809"}}},"source":["model = Model(inputs=input, outputs=output)\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 32, 32, 35)   945         ['input_1[0][0]']                \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 32, 32, 35)  140         ['conv2d[0][0]']                 \n"," alization)                                                                                       \n","                                                                                                  \n"," activation (Activation)        (None, 32, 32, 35)   0           ['batch_normalization[0][0]']    \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 32, 32, 35)   11025       ['activation[0][0]']             \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 70)   0           ['conv2d[0][0]',                 \n","                                                                  'conv2d_1[0][0]']               \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 32, 32, 70)  280         ['concatenate[0][0]']            \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_1 (Activation)      (None, 32, 32, 70)   0           ['batch_normalization_1[0][0]']  \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 32, 32, 35)   22050       ['activation_1[0][0]']           \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 32, 32, 105)  0           ['concatenate[0][0]',            \n","                                                                  'conv2d_2[0][0]']               \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 32, 32, 105)  420        ['concatenate_1[0][0]']          \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_2 (Activation)      (None, 32, 32, 105)  0           ['batch_normalization_2[0][0]']  \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 32, 32, 35)   33075       ['activation_2[0][0]']           \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 32, 32, 140)  0           ['concatenate_1[0][0]',          \n","                                                                  'conv2d_3[0][0]']               \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 32, 32, 140)  560        ['concatenate_2[0][0]']          \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_3 (Activation)      (None, 32, 32, 140)  0           ['batch_normalization_3[0][0]']  \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 32, 32, 35)   44100       ['activation_3[0][0]']           \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 32, 32, 175)  0           ['concatenate_2[0][0]',          \n","                                                                  'conv2d_4[0][0]']               \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 32, 32, 175)  700        ['concatenate_3[0][0]']          \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 32, 32, 175)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 32, 32, 35)   55125       ['activation_4[0][0]']           \n","                                                                                                  \n"," concatenate_4 (Concatenate)    (None, 32, 32, 210)  0           ['concatenate_3[0][0]',          \n","                                                                  'conv2d_5[0][0]']               \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 32, 32, 210)  840        ['concatenate_4[0][0]']          \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 32, 32, 210)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 32, 32, 35)   66150       ['activation_5[0][0]']           \n","                                                                                                  \n"," concatenate_5 (Concatenate)    (None, 32, 32, 245)  0           ['concatenate_4[0][0]',          \n","                                                                  'conv2d_6[0][0]']               \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 32, 32, 245)  980        ['concatenate_5[0][0]']          \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_6 (Activation)      (None, 32, 32, 245)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 32, 32, 35)   8575        ['activation_6[0][0]']           \n","                                                                                                  \n"," average_pooling2d (AveragePool  (None, 16, 16, 35)  0           ['conv2d_7[0][0]']               \n"," ing2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 16, 16, 35)  140         ['average_pooling2d[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_7 (Activation)      (None, 16, 16, 35)   0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 16, 16, 35)   11025       ['activation_7[0][0]']           \n","                                                                                                  \n"," concatenate_6 (Concatenate)    (None, 16, 16, 70)   0           ['average_pooling2d[0][0]',      \n","                                                                  'conv2d_8[0][0]']               \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 16, 16, 70)  280         ['concatenate_6[0][0]']          \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_8 (Activation)      (None, 16, 16, 70)   0           ['batch_normalization_8[0][0]']  \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 16, 16, 35)   22050       ['activation_8[0][0]']           \n","                                                                                                  \n"," concatenate_7 (Concatenate)    (None, 16, 16, 105)  0           ['concatenate_6[0][0]',          \n","                                                                  'conv2d_9[0][0]']               \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 16, 16, 105)  420        ['concatenate_7[0][0]']          \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_9 (Activation)      (None, 16, 16, 105)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 16, 16, 35)   33075       ['activation_9[0][0]']           \n","                                                                                                  \n"," concatenate_8 (Concatenate)    (None, 16, 16, 140)  0           ['concatenate_7[0][0]',          \n","                                                                  'conv2d_10[0][0]']              \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 16, 16, 140)  560        ['concatenate_8[0][0]']          \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_10 (Activation)     (None, 16, 16, 140)  0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 16, 16, 35)   44100       ['activation_10[0][0]']          \n","                                                                                                  \n"," concatenate_9 (Concatenate)    (None, 16, 16, 175)  0           ['concatenate_8[0][0]',          \n","                                                                  'conv2d_11[0][0]']              \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 16, 16, 175)  700        ['concatenate_9[0][0]']          \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_11 (Activation)     (None, 16, 16, 175)  0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 16, 16, 35)   55125       ['activation_11[0][0]']          \n","                                                                                                  \n"," concatenate_10 (Concatenate)   (None, 16, 16, 210)  0           ['concatenate_9[0][0]',          \n","                                                                  'conv2d_12[0][0]']              \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 16, 16, 210)  840        ['concatenate_10[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_12 (Activation)     (None, 16, 16, 210)  0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 16, 16, 35)   66150       ['activation_12[0][0]']          \n","                                                                                                  \n"," concatenate_11 (Concatenate)   (None, 16, 16, 245)  0           ['concatenate_10[0][0]',         \n","                                                                  'conv2d_13[0][0]']              \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 16, 16, 245)  980        ['concatenate_11[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_13 (Activation)     (None, 16, 16, 245)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 16, 16, 35)   8575        ['activation_13[0][0]']          \n","                                                                                                  \n"," average_pooling2d_1 (AveragePo  (None, 8, 8, 35)    0           ['conv2d_14[0][0]']              \n"," oling2D)                                                                                         \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 8, 8, 35)    140         ['average_pooling2d_1[0][0]']    \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_14 (Activation)     (None, 8, 8, 35)     0           ['batch_normalization_14[0][0]'] \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 8, 8, 35)     11025       ['activation_14[0][0]']          \n","                                                                                                  \n"," concatenate_12 (Concatenate)   (None, 8, 8, 70)     0           ['average_pooling2d_1[0][0]',    \n","                                                                  'conv2d_15[0][0]']              \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 8, 8, 70)    280         ['concatenate_12[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_15 (Activation)     (None, 8, 8, 70)     0           ['batch_normalization_15[0][0]'] \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 8, 8, 35)     22050       ['activation_15[0][0]']          \n","                                                                                                  \n"," concatenate_13 (Concatenate)   (None, 8, 8, 105)    0           ['concatenate_12[0][0]',         \n","                                                                  'conv2d_16[0][0]']              \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 8, 8, 105)   420         ['concatenate_13[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_16 (Activation)     (None, 8, 8, 105)    0           ['batch_normalization_16[0][0]'] \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 8, 8, 35)     33075       ['activation_16[0][0]']          \n","                                                                                                  \n"," concatenate_14 (Concatenate)   (None, 8, 8, 140)    0           ['concatenate_13[0][0]',         \n","                                                                  'conv2d_17[0][0]']              \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 8, 8, 140)   560         ['concatenate_14[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_17 (Activation)     (None, 8, 8, 140)    0           ['batch_normalization_17[0][0]'] \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 8, 8, 35)     44100       ['activation_17[0][0]']          \n","                                                                                                  \n"," concatenate_15 (Concatenate)   (None, 8, 8, 175)    0           ['concatenate_14[0][0]',         \n","                                                                  'conv2d_18[0][0]']              \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 8, 8, 175)   700         ['concatenate_15[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_18 (Activation)     (None, 8, 8, 175)    0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 8, 8, 35)     55125       ['activation_18[0][0]']          \n","                                                                                                  \n"," concatenate_16 (Concatenate)   (None, 8, 8, 210)    0           ['concatenate_15[0][0]',         \n","                                                                  'conv2d_19[0][0]']              \n","                                                                                                  \n"," batch_normalization_19 (BatchN  (None, 8, 8, 210)   840         ['concatenate_16[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_19 (Activation)     (None, 8, 8, 210)    0           ['batch_normalization_19[0][0]'] \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 8, 8, 35)     66150       ['activation_19[0][0]']          \n","                                                                                                  \n"," concatenate_17 (Concatenate)   (None, 8, 8, 245)    0           ['concatenate_16[0][0]',         \n","                                                                  'conv2d_20[0][0]']              \n","                                                                                                  \n"," batch_normalization_20 (BatchN  (None, 8, 8, 245)   980         ['concatenate_17[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_20 (Activation)     (None, 8, 8, 245)    0           ['batch_normalization_20[0][0]'] \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 8, 8, 35)     8575        ['activation_20[0][0]']          \n","                                                                                                  \n"," average_pooling2d_2 (AveragePo  (None, 4, 4, 35)    0           ['conv2d_21[0][0]']              \n"," oling2D)                                                                                         \n","                                                                                                  \n"," batch_normalization_21 (BatchN  (None, 4, 4, 35)    140         ['average_pooling2d_2[0][0]']    \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_21 (Activation)     (None, 4, 4, 35)     0           ['batch_normalization_21[0][0]'] \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 4, 4, 35)     11025       ['activation_21[0][0]']          \n","                                                                                                  \n"," concatenate_18 (Concatenate)   (None, 4, 4, 70)     0           ['average_pooling2d_2[0][0]',    \n","                                                                  'conv2d_22[0][0]']              \n","                                                                                                  \n"," batch_normalization_22 (BatchN  (None, 4, 4, 70)    280         ['concatenate_18[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_22 (Activation)     (None, 4, 4, 70)     0           ['batch_normalization_22[0][0]'] \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 4, 4, 35)     22050       ['activation_22[0][0]']          \n","                                                                                                  \n"," concatenate_19 (Concatenate)   (None, 4, 4, 105)    0           ['concatenate_18[0][0]',         \n","                                                                  'conv2d_23[0][0]']              \n","                                                                                                  \n"," batch_normalization_23 (BatchN  (None, 4, 4, 105)   420         ['concatenate_19[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_23 (Activation)     (None, 4, 4, 105)    0           ['batch_normalization_23[0][0]'] \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 4, 4, 35)     33075       ['activation_23[0][0]']          \n","                                                                                                  \n"," concatenate_20 (Concatenate)   (None, 4, 4, 140)    0           ['concatenate_19[0][0]',         \n","                                                                  'conv2d_24[0][0]']              \n","                                                                                                  \n"," batch_normalization_24 (BatchN  (None, 4, 4, 140)   560         ['concatenate_20[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_24 (Activation)     (None, 4, 4, 140)    0           ['batch_normalization_24[0][0]'] \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 4, 4, 35)     44100       ['activation_24[0][0]']          \n","                                                                                                  \n"," concatenate_21 (Concatenate)   (None, 4, 4, 175)    0           ['concatenate_20[0][0]',         \n","                                                                  'conv2d_25[0][0]']              \n","                                                                                                  \n"," batch_normalization_25 (BatchN  (None, 4, 4, 175)   700         ['concatenate_21[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_25 (Activation)     (None, 4, 4, 175)    0           ['batch_normalization_25[0][0]'] \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 4, 4, 35)     55125       ['activation_25[0][0]']          \n","                                                                                                  \n"," concatenate_22 (Concatenate)   (None, 4, 4, 210)    0           ['concatenate_21[0][0]',         \n","                                                                  'conv2d_26[0][0]']              \n","                                                                                                  \n"," batch_normalization_26 (BatchN  (None, 4, 4, 210)   840         ['concatenate_22[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_26 (Activation)     (None, 4, 4, 210)    0           ['batch_normalization_26[0][0]'] \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 4, 4, 35)     66150       ['activation_26[0][0]']          \n","                                                                                                  \n"," concatenate_23 (Concatenate)   (None, 4, 4, 245)    0           ['concatenate_22[0][0]',         \n","                                                                  'conv2d_27[0][0]']              \n","                                                                                                  \n"," batch_normalization_27 (BatchN  (None, 4, 4, 245)   980         ['concatenate_23[0][0]']         \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_27 (Activation)     (None, 4, 4, 245)    0           ['batch_normalization_27[0][0]'] \n","                                                                                                  \n"," average_pooling2d_3 (AveragePo  (None, 2, 2, 245)   0           ['activation_27[0][0]']          \n"," oling2D)                                                                                         \n","                                                                                                  \n"," flatten (Flatten)              (None, 980)          0           ['average_pooling2d_3[0][0]']    \n","                                                                                                  \n"," dense (Dense)                  (None, 10)           9810        ['flatten[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 978,260\n","Trainable params: 970,420\n","Non-trainable params: 7,840\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"_kWhC0Cj6zYq"},"source":["model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n","                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n","                metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cd7_cxUwYjkY"},"source":["#lrschedule = LearningRateScheduler(changeLearningRate, verbose=1)\n","filepath = 'model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5'\n","checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\n","callback_list = [checkpoint]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiJGybtJ6xSp","outputId":"655bced2-9ae2-4f32-eebe-4ef0bb6107b4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687001719591,"user_tz":-330,"elapsed":3124691,"user":{"displayName":"Subhajit Sarkar","userId":"12932209816484918809"}}},"source":["model.fit(traindataGen.flow(X_train, y_train, batch_size=128), steps_per_epoch=len(X_train) / 128, epochs=epochs, validation_data=testdataGen.flow(X_test, y_test), callbacks=callback_list)"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/300\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/backend.py:5561: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"output_type":"stream","name":"stdout","text":["391/390 [==============================] - ETA: 0s - loss: 1.6051 - accuracy: 0.4134\n","Epoch 1: val_accuracy improved from -inf to 0.34000, saving model to model_save/weights-01-0.3400.hdf5\n","390/390 [==============================] - 84s 164ms/step - loss: 1.6051 - accuracy: 0.4134 - val_loss: 1.8068 - val_accuracy: 0.3400\n","Epoch 2/300\n","391/390 [==============================] - ETA: 0s - loss: 1.2609 - accuracy: 0.5457\n","Epoch 2: val_accuracy improved from 0.34000 to 0.54020, saving model to model_save/weights-02-0.5402.hdf5\n","390/390 [==============================] - 60s 154ms/step - loss: 1.2609 - accuracy: 0.5457 - val_loss: 1.4040 - val_accuracy: 0.5402\n","Epoch 3/300\n","391/390 [==============================] - ETA: 0s - loss: 1.0684 - accuracy: 0.6198\n","Epoch 3: val_accuracy improved from 0.54020 to 0.61480, saving model to model_save/weights-03-0.6148.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 1.0684 - accuracy: 0.6198 - val_loss: 1.0812 - val_accuracy: 0.6148\n","Epoch 4/300\n","391/390 [==============================] - ETA: 0s - loss: 0.9479 - accuracy: 0.6635\n","Epoch 4: val_accuracy improved from 0.61480 to 0.63640, saving model to model_save/weights-04-0.6364.hdf5\n","390/390 [==============================] - 61s 157ms/step - loss: 0.9479 - accuracy: 0.6635 - val_loss: 1.0665 - val_accuracy: 0.6364\n","Epoch 5/300\n","391/390 [==============================] - ETA: 0s - loss: 0.8561 - accuracy: 0.6965\n","Epoch 5: val_accuracy did not improve from 0.63640\n","390/390 [==============================] - 62s 157ms/step - loss: 0.8561 - accuracy: 0.6965 - val_loss: 1.1343 - val_accuracy: 0.6316\n","Epoch 6/300\n","391/390 [==============================] - ETA: 0s - loss: 0.7968 - accuracy: 0.7203\n","Epoch 6: val_accuracy improved from 0.63640 to 0.66480, saving model to model_save/weights-06-0.6648.hdf5\n","390/390 [==============================] - 62s 159ms/step - loss: 0.7968 - accuracy: 0.7203 - val_loss: 1.0393 - val_accuracy: 0.6648\n","Epoch 7/300\n","391/390 [==============================] - ETA: 0s - loss: 0.7452 - accuracy: 0.7398\n","Epoch 7: val_accuracy improved from 0.66480 to 0.69530, saving model to model_save/weights-07-0.6953.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.7452 - accuracy: 0.7398 - val_loss: 0.9379 - val_accuracy: 0.6953\n","Epoch 8/300\n","391/390 [==============================] - ETA: 0s - loss: 0.6977 - accuracy: 0.7540\n","Epoch 8: val_accuracy improved from 0.69530 to 0.74240, saving model to model_save/weights-08-0.7424.hdf5\n","390/390 [==============================] - 63s 161ms/step - loss: 0.6977 - accuracy: 0.7540 - val_loss: 0.7696 - val_accuracy: 0.7424\n","Epoch 9/300\n","391/390 [==============================] - ETA: 0s - loss: 0.6645 - accuracy: 0.7681\n","Epoch 9: val_accuracy improved from 0.74240 to 0.75120, saving model to model_save/weights-09-0.7512.hdf5\n","390/390 [==============================] - 61s 157ms/step - loss: 0.6645 - accuracy: 0.7681 - val_loss: 0.7649 - val_accuracy: 0.7512\n","Epoch 10/300\n","391/390 [==============================] - ETA: 0s - loss: 0.6292 - accuracy: 0.7815\n","Epoch 10: val_accuracy did not improve from 0.75120\n","390/390 [==============================] - 62s 157ms/step - loss: 0.6292 - accuracy: 0.7815 - val_loss: 0.9764 - val_accuracy: 0.6935\n","Epoch 11/300\n","391/390 [==============================] - ETA: 0s - loss: 0.6021 - accuracy: 0.7892\n","Epoch 11: val_accuracy improved from 0.75120 to 0.76250, saving model to model_save/weights-11-0.7625.hdf5\n","390/390 [==============================] - 61s 157ms/step - loss: 0.6021 - accuracy: 0.7892 - val_loss: 0.7499 - val_accuracy: 0.7625\n","Epoch 12/300\n","391/390 [==============================] - ETA: 0s - loss: 0.5840 - accuracy: 0.7969\n","Epoch 12: val_accuracy did not improve from 0.76250\n","390/390 [==============================] - 62s 160ms/step - loss: 0.5840 - accuracy: 0.7969 - val_loss: 1.3320 - val_accuracy: 0.6321\n","Epoch 13/300\n","391/390 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.8080\n","Epoch 13: val_accuracy improved from 0.76250 to 0.78530, saving model to model_save/weights-13-0.7853.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.5540 - accuracy: 0.8080 - val_loss: 0.6577 - val_accuracy: 0.7853\n","Epoch 14/300\n","391/390 [==============================] - ETA: 0s - loss: 0.5368 - accuracy: 0.8130\n","Epoch 14: val_accuracy did not improve from 0.78530\n","390/390 [==============================] - 61s 156ms/step - loss: 0.5368 - accuracy: 0.8130 - val_loss: 0.7665 - val_accuracy: 0.7510\n","Epoch 15/300\n","391/390 [==============================] - ETA: 0s - loss: 0.5202 - accuracy: 0.8181\n","Epoch 15: val_accuracy improved from 0.78530 to 0.80010, saving model to model_save/weights-15-0.8001.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.5202 - accuracy: 0.8181 - val_loss: 0.6304 - val_accuracy: 0.8001\n","Epoch 16/300\n","391/390 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.8237\n","Epoch 16: val_accuracy did not improve from 0.80010\n","390/390 [==============================] - 61s 157ms/step - loss: 0.5063 - accuracy: 0.8237 - val_loss: 0.6900 - val_accuracy: 0.7763\n","Epoch 17/300\n","391/390 [==============================] - ETA: 0s - loss: 0.4887 - accuracy: 0.8290\n","Epoch 17: val_accuracy did not improve from 0.80010\n","390/390 [==============================] - 62s 158ms/step - loss: 0.4887 - accuracy: 0.8290 - val_loss: 0.6245 - val_accuracy: 0.7991\n","Epoch 18/300\n","391/390 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.8361\n","Epoch 18: val_accuracy did not improve from 0.80010\n","390/390 [==============================] - 61s 157ms/step - loss: 0.4749 - accuracy: 0.8361 - val_loss: 0.6687 - val_accuracy: 0.7848\n","Epoch 19/300\n","391/390 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.8380\n","Epoch 19: val_accuracy improved from 0.80010 to 0.81510, saving model to model_save/weights-19-0.8151.hdf5\n","390/390 [==============================] - 61s 157ms/step - loss: 0.4696 - accuracy: 0.8380 - val_loss: 0.5759 - val_accuracy: 0.8151\n","Epoch 20/300\n","391/390 [==============================] - ETA: 0s - loss: 0.4507 - accuracy: 0.8448\n","Epoch 20: val_accuracy did not improve from 0.81510\n","390/390 [==============================] - 61s 157ms/step - loss: 0.4507 - accuracy: 0.8448 - val_loss: 0.6802 - val_accuracy: 0.7812\n","Epoch 21/300\n","391/390 [==============================] - ETA: 0s - loss: 0.4381 - accuracy: 0.8475\n","Epoch 21: val_accuracy did not improve from 0.81510\n","390/390 [==============================] - 61s 156ms/step - loss: 0.4381 - accuracy: 0.8475 - val_loss: 0.6332 - val_accuracy: 0.7864\n","Epoch 22/300\n","391/390 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8502\n","Epoch 22: val_accuracy did not improve from 0.81510\n","390/390 [==============================] - 61s 157ms/step - loss: 0.4298 - accuracy: 0.8502 - val_loss: 0.5982 - val_accuracy: 0.8090\n","Epoch 23/300\n","391/390 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.8541\n","Epoch 23: val_accuracy improved from 0.81510 to 0.83240, saving model to model_save/weights-23-0.8324.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.4196 - accuracy: 0.8541 - val_loss: 0.5154 - val_accuracy: 0.8324\n","Epoch 24/300\n","391/390 [==============================] - ETA: 0s - loss: 0.4082 - accuracy: 0.8576\n","Epoch 24: val_accuracy improved from 0.83240 to 0.83950, saving model to model_save/weights-24-0.8395.hdf5\n","390/390 [==============================] - 62s 157ms/step - loss: 0.4082 - accuracy: 0.8576 - val_loss: 0.4939 - val_accuracy: 0.8395\n","Epoch 25/300\n","391/390 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8609\n","Epoch 25: val_accuracy did not improve from 0.83950\n","390/390 [==============================] - 61s 156ms/step - loss: 0.4001 - accuracy: 0.8609 - val_loss: 0.5736 - val_accuracy: 0.8209\n","Epoch 26/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3923 - accuracy: 0.8636\n","Epoch 26: val_accuracy did not improve from 0.83950\n","390/390 [==============================] - 62s 158ms/step - loss: 0.3923 - accuracy: 0.8636 - val_loss: 0.7837 - val_accuracy: 0.7737\n","Epoch 27/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3844 - accuracy: 0.8650\n","Epoch 27: val_accuracy did not improve from 0.83950\n","390/390 [==============================] - 62s 159ms/step - loss: 0.3844 - accuracy: 0.8650 - val_loss: 0.5066 - val_accuracy: 0.8375\n","Epoch 28/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3797 - accuracy: 0.8674\n","Epoch 28: val_accuracy did not improve from 0.83950\n","390/390 [==============================] - 62s 158ms/step - loss: 0.3797 - accuracy: 0.8674 - val_loss: 0.6193 - val_accuracy: 0.8132\n","Epoch 29/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.8714\n","Epoch 29: val_accuracy improved from 0.83950 to 0.84640, saving model to model_save/weights-29-0.8464.hdf5\n","390/390 [==============================] - 62s 157ms/step - loss: 0.3703 - accuracy: 0.8714 - val_loss: 0.4750 - val_accuracy: 0.8464\n","Epoch 30/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3681 - accuracy: 0.8712\n","Epoch 30: val_accuracy did not improve from 0.84640\n","390/390 [==============================] - 61s 156ms/step - loss: 0.3681 - accuracy: 0.8712 - val_loss: 0.5816 - val_accuracy: 0.8180\n","Epoch 31/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3581 - accuracy: 0.8750\n","Epoch 31: val_accuracy did not improve from 0.84640\n","390/390 [==============================] - 61s 156ms/step - loss: 0.3581 - accuracy: 0.8750 - val_loss: 0.4761 - val_accuracy: 0.8445\n","Epoch 32/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.8785\n","Epoch 32: val_accuracy did not improve from 0.84640\n","390/390 [==============================] - 62s 159ms/step - loss: 0.3524 - accuracy: 0.8785 - val_loss: 0.5391 - val_accuracy: 0.8382\n","Epoch 33/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.8801\n","Epoch 33: val_accuracy did not improve from 0.84640\n","390/390 [==============================] - 61s 157ms/step - loss: 0.3455 - accuracy: 0.8801 - val_loss: 0.5238 - val_accuracy: 0.8386\n","Epoch 34/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3391 - accuracy: 0.8808\n","Epoch 34: val_accuracy did not improve from 0.84640\n","390/390 [==============================] - 61s 157ms/step - loss: 0.3391 - accuracy: 0.8808 - val_loss: 0.5740 - val_accuracy: 0.8296\n","Epoch 35/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3370 - accuracy: 0.8825\n","Epoch 35: val_accuracy did not improve from 0.84640\n","390/390 [==============================] - 61s 156ms/step - loss: 0.3370 - accuracy: 0.8825 - val_loss: 0.4930 - val_accuracy: 0.8453\n","Epoch 36/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.8834\n","Epoch 36: val_accuracy did not improve from 0.84640\n","390/390 [==============================] - 61s 156ms/step - loss: 0.3296 - accuracy: 0.8834 - val_loss: 0.5739 - val_accuracy: 0.8272\n","Epoch 37/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.8866\n","Epoch 37: val_accuracy improved from 0.84640 to 0.86620, saving model to model_save/weights-37-0.8662.hdf5\n","390/390 [==============================] - 62s 157ms/step - loss: 0.3240 - accuracy: 0.8866 - val_loss: 0.4252 - val_accuracy: 0.8662\n","Epoch 38/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3203 - accuracy: 0.8872\n","Epoch 38: val_accuracy did not improve from 0.86620\n","390/390 [==============================] - 61s 157ms/step - loss: 0.3203 - accuracy: 0.8872 - val_loss: 0.7152 - val_accuracy: 0.8025\n","Epoch 39/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.8905\n","Epoch 39: val_accuracy improved from 0.86620 to 0.87920, saving model to model_save/weights-39-0.8792.hdf5\n","390/390 [==============================] - 62s 159ms/step - loss: 0.3127 - accuracy: 0.8905 - val_loss: 0.3636 - val_accuracy: 0.8792\n","Epoch 40/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.8902\n","Epoch 40: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 61s 156ms/step - loss: 0.3105 - accuracy: 0.8902 - val_loss: 0.5706 - val_accuracy: 0.8264\n","Epoch 41/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8917\n","Epoch 41: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 61s 157ms/step - loss: 0.3049 - accuracy: 0.8917 - val_loss: 0.4174 - val_accuracy: 0.8686\n","Epoch 42/300\n","391/390 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8953\n","Epoch 42: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 62s 158ms/step - loss: 0.3002 - accuracy: 0.8953 - val_loss: 0.3847 - val_accuracy: 0.8745\n","Epoch 43/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.8963\n","Epoch 43: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 62s 157ms/step - loss: 0.2967 - accuracy: 0.8963 - val_loss: 0.4267 - val_accuracy: 0.8654\n","Epoch 44/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2939 - accuracy: 0.8967\n","Epoch 44: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2939 - accuracy: 0.8967 - val_loss: 0.4485 - val_accuracy: 0.8608\n","Epoch 45/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.8994\n","Epoch 45: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2888 - accuracy: 0.8994 - val_loss: 0.4155 - val_accuracy: 0.8736\n","Epoch 46/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9007\n","Epoch 46: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 61s 156ms/step - loss: 0.2841 - accuracy: 0.9007 - val_loss: 0.3943 - val_accuracy: 0.8724\n","Epoch 47/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2809 - accuracy: 0.9017\n","Epoch 47: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2809 - accuracy: 0.9017 - val_loss: 0.4463 - val_accuracy: 0.8653\n","Epoch 48/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.9016\n","Epoch 48: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 63s 160ms/step - loss: 0.2816 - accuracy: 0.9016 - val_loss: 0.4228 - val_accuracy: 0.8680\n","Epoch 49/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.9035\n","Epoch 49: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 63s 160ms/step - loss: 0.2775 - accuracy: 0.9035 - val_loss: 0.4836 - val_accuracy: 0.8547\n","Epoch 50/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9052\n","Epoch 50: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 62s 158ms/step - loss: 0.2711 - accuracy: 0.9052 - val_loss: 0.4602 - val_accuracy: 0.8611\n","Epoch 51/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.9025\n","Epoch 51: val_accuracy did not improve from 0.87920\n","390/390 [==============================] - 62s 158ms/step - loss: 0.2742 - accuracy: 0.9025 - val_loss: 0.5098 - val_accuracy: 0.8440\n","Epoch 52/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.9057\n","Epoch 52: val_accuracy improved from 0.87920 to 0.88450, saving model to model_save/weights-52-0.8845.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.2666 - accuracy: 0.9057 - val_loss: 0.3718 - val_accuracy: 0.8845\n","Epoch 53/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.9073\n","Epoch 53: val_accuracy did not improve from 0.88450\n","390/390 [==============================] - 62s 160ms/step - loss: 0.2633 - accuracy: 0.9073 - val_loss: 0.4456 - val_accuracy: 0.8632\n","Epoch 54/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9094\n","Epoch 54: val_accuracy improved from 0.88450 to 0.88980, saving model to model_save/weights-54-0.8898.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.2577 - accuracy: 0.9094 - val_loss: 0.3492 - val_accuracy: 0.8898\n","Epoch 55/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9095\n","Epoch 55: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 62s 157ms/step - loss: 0.2557 - accuracy: 0.9095 - val_loss: 0.4510 - val_accuracy: 0.8611\n","Epoch 56/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9131\n","Epoch 56: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 63s 161ms/step - loss: 0.2489 - accuracy: 0.9131 - val_loss: 0.4568 - val_accuracy: 0.8622\n","Epoch 57/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9110\n","Epoch 57: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 62s 158ms/step - loss: 0.2532 - accuracy: 0.9110 - val_loss: 0.4795 - val_accuracy: 0.8595\n","Epoch 58/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.9119\n","Epoch 58: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 62s 158ms/step - loss: 0.2507 - accuracy: 0.9119 - val_loss: 0.3683 - val_accuracy: 0.8871\n","Epoch 59/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9121\n","Epoch 59: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2490 - accuracy: 0.9121 - val_loss: 0.4415 - val_accuracy: 0.8714\n","Epoch 60/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.9145\n","Epoch 60: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2398 - accuracy: 0.9145 - val_loss: 0.4213 - val_accuracy: 0.8725\n","Epoch 61/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2356 - accuracy: 0.9169\n","Epoch 61: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 62s 158ms/step - loss: 0.2356 - accuracy: 0.9169 - val_loss: 0.4370 - val_accuracy: 0.8652\n","Epoch 62/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2353 - accuracy: 0.9174\n","Epoch 62: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 63s 160ms/step - loss: 0.2353 - accuracy: 0.9174 - val_loss: 0.3851 - val_accuracy: 0.8841\n","Epoch 63/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9159\n","Epoch 63: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2347 - accuracy: 0.9159 - val_loss: 0.3978 - val_accuracy: 0.8806\n","Epoch 64/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.9180\n","Epoch 64: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 63s 160ms/step - loss: 0.2341 - accuracy: 0.9180 - val_loss: 0.3816 - val_accuracy: 0.8821\n","Epoch 65/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.9183\n","Epoch 65: val_accuracy did not improve from 0.88980\n","390/390 [==============================] - 61s 156ms/step - loss: 0.2298 - accuracy: 0.9183 - val_loss: 0.4536 - val_accuracy: 0.8706\n","Epoch 66/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.9203\n","Epoch 66: val_accuracy improved from 0.88980 to 0.89420, saving model to model_save/weights-66-0.8942.hdf5\n","390/390 [==============================] - 63s 161ms/step - loss: 0.2256 - accuracy: 0.9203 - val_loss: 0.3396 - val_accuracy: 0.8942\n","Epoch 67/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9208\n","Epoch 67: val_accuracy did not improve from 0.89420\n","390/390 [==============================] - 63s 160ms/step - loss: 0.2248 - accuracy: 0.9208 - val_loss: 0.3680 - val_accuracy: 0.8918\n","Epoch 68/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.9216\n","Epoch 68: val_accuracy did not improve from 0.89420\n","390/390 [==============================] - 62s 158ms/step - loss: 0.2219 - accuracy: 0.9216 - val_loss: 0.4009 - val_accuracy: 0.8798\n","Epoch 69/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9226\n","Epoch 69: val_accuracy did not improve from 0.89420\n","390/390 [==============================] - 61s 156ms/step - loss: 0.2195 - accuracy: 0.9226 - val_loss: 0.4141 - val_accuracy: 0.8783\n","Epoch 70/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9237\n","Epoch 70: val_accuracy did not improve from 0.89420\n","390/390 [==============================] - 62s 157ms/step - loss: 0.2138 - accuracy: 0.9237 - val_loss: 0.4087 - val_accuracy: 0.8841\n","Epoch 71/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 0.9267\n","Epoch 71: val_accuracy improved from 0.89420 to 0.89640, saving model to model_save/weights-71-0.8964.hdf5\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2136 - accuracy: 0.9267 - val_loss: 0.3437 - val_accuracy: 0.8964\n","Epoch 72/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9250\n","Epoch 72: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2146 - accuracy: 0.9250 - val_loss: 0.4554 - val_accuracy: 0.8710\n","Epoch 73/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9258\n","Epoch 73: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2103 - accuracy: 0.9258 - val_loss: 0.3641 - val_accuracy: 0.8895\n","Epoch 74/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2049 - accuracy: 0.9286\n","Epoch 74: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 61s 156ms/step - loss: 0.2049 - accuracy: 0.9286 - val_loss: 0.3945 - val_accuracy: 0.8851\n","Epoch 75/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2052 - accuracy: 0.9286\n","Epoch 75: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 62s 158ms/step - loss: 0.2052 - accuracy: 0.9286 - val_loss: 0.6113 - val_accuracy: 0.8381\n","Epoch 76/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2072 - accuracy: 0.9265\n","Epoch 76: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2072 - accuracy: 0.9265 - val_loss: 0.4317 - val_accuracy: 0.8711\n","Epoch 77/300\n","391/390 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9265\n","Epoch 77: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 61s 157ms/step - loss: 0.2059 - accuracy: 0.9265 - val_loss: 0.4434 - val_accuracy: 0.8710\n","Epoch 78/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1977 - accuracy: 0.9305\n","Epoch 78: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1977 - accuracy: 0.9305 - val_loss: 0.3550 - val_accuracy: 0.8906\n","Epoch 79/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9301\n","Epoch 79: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 62s 157ms/step - loss: 0.1970 - accuracy: 0.9301 - val_loss: 0.5298 - val_accuracy: 0.8597\n","Epoch 80/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1989 - accuracy: 0.9307\n","Epoch 80: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 61s 156ms/step - loss: 0.1989 - accuracy: 0.9307 - val_loss: 0.3678 - val_accuracy: 0.8904\n","Epoch 81/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9318\n","Epoch 81: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1920 - accuracy: 0.9318 - val_loss: 0.4145 - val_accuracy: 0.8813\n","Epoch 82/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1914 - accuracy: 0.9319\n","Epoch 82: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1914 - accuracy: 0.9319 - val_loss: 0.3678 - val_accuracy: 0.8886\n","Epoch 83/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9327\n","Epoch 83: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1926 - accuracy: 0.9327 - val_loss: 0.4269 - val_accuracy: 0.8774\n","Epoch 84/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9338\n","Epoch 84: val_accuracy did not improve from 0.89640\n","390/390 [==============================] - 62s 157ms/step - loss: 0.1915 - accuracy: 0.9338 - val_loss: 0.3889 - val_accuracy: 0.8838\n","Epoch 85/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9340\n","Epoch 85: val_accuracy improved from 0.89640 to 0.89950, saving model to model_save/weights-85-0.8995.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1891 - accuracy: 0.9340 - val_loss: 0.3366 - val_accuracy: 0.8995\n","Epoch 86/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.9337\n","Epoch 86: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1878 - accuracy: 0.9337 - val_loss: 0.4102 - val_accuracy: 0.8821\n","Epoch 87/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.9358\n","Epoch 87: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1867 - accuracy: 0.9358 - val_loss: 0.3469 - val_accuracy: 0.8976\n","Epoch 88/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9379\n","Epoch 88: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1789 - accuracy: 0.9379 - val_loss: 0.3506 - val_accuracy: 0.8955\n","Epoch 89/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9349\n","Epoch 89: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 156ms/step - loss: 0.1840 - accuracy: 0.9349 - val_loss: 0.4879 - val_accuracy: 0.8673\n","Epoch 90/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.9365\n","Epoch 90: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1802 - accuracy: 0.9365 - val_loss: 0.4706 - val_accuracy: 0.8722\n","Epoch 91/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9377\n","Epoch 91: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1740 - accuracy: 0.9377 - val_loss: 0.4286 - val_accuracy: 0.8816\n","Epoch 92/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9386\n","Epoch 92: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1749 - accuracy: 0.9386 - val_loss: 0.3676 - val_accuracy: 0.8930\n","Epoch 93/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1752 - accuracy: 0.9378\n","Epoch 93: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1752 - accuracy: 0.9378 - val_loss: 0.4244 - val_accuracy: 0.8811\n","Epoch 94/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9378\n","Epoch 94: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 63s 162ms/step - loss: 0.1718 - accuracy: 0.9378 - val_loss: 0.3418 - val_accuracy: 0.8955\n","Epoch 95/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9387\n","Epoch 95: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1760 - accuracy: 0.9387 - val_loss: 0.3750 - val_accuracy: 0.8905\n","Epoch 96/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9384\n","Epoch 96: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 156ms/step - loss: 0.1729 - accuracy: 0.9384 - val_loss: 0.3682 - val_accuracy: 0.8924\n","Epoch 97/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.9408\n","Epoch 97: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 63s 160ms/step - loss: 0.1690 - accuracy: 0.9408 - val_loss: 0.5126 - val_accuracy: 0.8629\n","Epoch 98/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9406\n","Epoch 98: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1697 - accuracy: 0.9406 - val_loss: 0.3872 - val_accuracy: 0.8910\n","Epoch 99/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9403\n","Epoch 99: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 156ms/step - loss: 0.1677 - accuracy: 0.9403 - val_loss: 0.3742 - val_accuracy: 0.8945\n","Epoch 100/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9409\n","Epoch 100: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 62s 159ms/step - loss: 0.1646 - accuracy: 0.9409 - val_loss: 0.3865 - val_accuracy: 0.8885\n","Epoch 101/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9433\n","Epoch 101: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1633 - accuracy: 0.9433 - val_loss: 0.3947 - val_accuracy: 0.8869\n","Epoch 102/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.9419\n","Epoch 102: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1638 - accuracy: 0.9419 - val_loss: 0.4076 - val_accuracy: 0.8855\n","Epoch 103/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9417\n","Epoch 103: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1637 - accuracy: 0.9417 - val_loss: 0.4162 - val_accuracy: 0.8839\n","Epoch 104/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.9448\n","Epoch 104: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 156ms/step - loss: 0.1574 - accuracy: 0.9448 - val_loss: 0.3653 - val_accuracy: 0.8945\n","Epoch 105/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9423\n","Epoch 105: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1628 - accuracy: 0.9423 - val_loss: 0.3711 - val_accuracy: 0.8914\n","Epoch 106/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9434\n","Epoch 106: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1577 - accuracy: 0.9434 - val_loss: 0.3869 - val_accuracy: 0.8939\n","Epoch 107/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.9445\n","Epoch 107: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1568 - accuracy: 0.9445 - val_loss: 0.4213 - val_accuracy: 0.8841\n","Epoch 108/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9458\n","Epoch 108: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1534 - accuracy: 0.9458 - val_loss: 0.3947 - val_accuracy: 0.8936\n","Epoch 109/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1567 - accuracy: 0.9434\n","Epoch 109: val_accuracy did not improve from 0.89950\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1567 - accuracy: 0.9434 - val_loss: 0.5442 - val_accuracy: 0.8608\n","Epoch 110/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9457\n","Epoch 110: val_accuracy improved from 0.89950 to 0.90860, saving model to model_save/weights-110-0.9086.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1557 - accuracy: 0.9457 - val_loss: 0.3215 - val_accuracy: 0.9086\n","Epoch 111/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9449\n","Epoch 111: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1551 - accuracy: 0.9449 - val_loss: 0.3859 - val_accuracy: 0.8959\n","Epoch 112/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9468\n","Epoch 112: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 62s 159ms/step - loss: 0.1512 - accuracy: 0.9468 - val_loss: 0.4671 - val_accuracy: 0.8770\n","Epoch 113/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9453\n","Epoch 113: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 62s 157ms/step - loss: 0.1543 - accuracy: 0.9453 - val_loss: 0.5094 - val_accuracy: 0.8618\n","Epoch 114/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9483\n","Epoch 114: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1449 - accuracy: 0.9483 - val_loss: 0.3636 - val_accuracy: 0.8984\n","Epoch 115/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9481\n","Epoch 115: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1474 - accuracy: 0.9481 - val_loss: 0.3760 - val_accuracy: 0.8951\n","Epoch 116/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9463\n","Epoch 116: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1505 - accuracy: 0.9463 - val_loss: 0.3520 - val_accuracy: 0.8994\n","Epoch 117/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9483\n","Epoch 117: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1472 - accuracy: 0.9483 - val_loss: 0.3346 - val_accuracy: 0.9067\n","Epoch 118/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9490\n","Epoch 118: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1427 - accuracy: 0.9490 - val_loss: 0.3282 - val_accuracy: 0.9054\n","Epoch 119/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9484\n","Epoch 119: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1434 - accuracy: 0.9484 - val_loss: 0.4091 - val_accuracy: 0.8862\n","Epoch 120/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9506\n","Epoch 120: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1377 - accuracy: 0.9506 - val_loss: 0.3933 - val_accuracy: 0.8939\n","Epoch 121/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9488\n","Epoch 121: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1436 - accuracy: 0.9488 - val_loss: 0.4248 - val_accuracy: 0.8890\n","Epoch 122/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9497\n","Epoch 122: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1403 - accuracy: 0.9497 - val_loss: 0.3706 - val_accuracy: 0.9009\n","Epoch 123/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9495\n","Epoch 123: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1416 - accuracy: 0.9495 - val_loss: 0.3421 - val_accuracy: 0.9028\n","Epoch 124/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9497\n","Epoch 124: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1417 - accuracy: 0.9497 - val_loss: 0.4744 - val_accuracy: 0.8794\n","Epoch 125/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9502\n","Epoch 125: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1403 - accuracy: 0.9502 - val_loss: 0.3274 - val_accuracy: 0.9078\n","Epoch 126/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9483\n","Epoch 126: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 61s 156ms/step - loss: 0.1422 - accuracy: 0.9483 - val_loss: 0.3165 - val_accuracy: 0.9060\n","Epoch 127/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9526\n","Epoch 127: val_accuracy did not improve from 0.90860\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1344 - accuracy: 0.9526 - val_loss: 0.4525 - val_accuracy: 0.8750\n","Epoch 128/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9536\n","Epoch 128: val_accuracy improved from 0.90860 to 0.90900, saving model to model_save/weights-128-0.9090.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1315 - accuracy: 0.9536 - val_loss: 0.3211 - val_accuracy: 0.9090\n","Epoch 129/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9521\n","Epoch 129: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1369 - accuracy: 0.9521 - val_loss: 0.4087 - val_accuracy: 0.8890\n","Epoch 130/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9522\n","Epoch 130: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1359 - accuracy: 0.9522 - val_loss: 0.3569 - val_accuracy: 0.8987\n","Epoch 131/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9548\n","Epoch 131: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1301 - accuracy: 0.9548 - val_loss: 0.4137 - val_accuracy: 0.8929\n","Epoch 132/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9536\n","Epoch 132: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1291 - accuracy: 0.9536 - val_loss: 0.3830 - val_accuracy: 0.8962\n","Epoch 133/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9511\n","Epoch 133: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1335 - accuracy: 0.9511 - val_loss: 0.3454 - val_accuracy: 0.9067\n","Epoch 134/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9549\n","Epoch 134: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1267 - accuracy: 0.9549 - val_loss: 0.4011 - val_accuracy: 0.8951\n","Epoch 135/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9544\n","Epoch 135: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1285 - accuracy: 0.9544 - val_loss: 0.3656 - val_accuracy: 0.9016\n","Epoch 136/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9559\n","Epoch 136: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 159ms/step - loss: 0.1255 - accuracy: 0.9559 - val_loss: 0.3882 - val_accuracy: 0.8975\n","Epoch 137/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9558\n","Epoch 137: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1247 - accuracy: 0.9558 - val_loss: 0.3832 - val_accuracy: 0.8979\n","Epoch 138/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9559\n","Epoch 138: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1260 - accuracy: 0.9559 - val_loss: 0.5033 - val_accuracy: 0.8770\n","Epoch 139/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9552\n","Epoch 139: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 63s 162ms/step - loss: 0.1261 - accuracy: 0.9552 - val_loss: 0.3441 - val_accuracy: 0.9052\n","Epoch 140/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9540\n","Epoch 140: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 63s 161ms/step - loss: 0.1270 - accuracy: 0.9540 - val_loss: 0.3742 - val_accuracy: 0.9028\n","Epoch 141/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9543\n","Epoch 141: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1278 - accuracy: 0.9543 - val_loss: 0.3788 - val_accuracy: 0.8976\n","Epoch 142/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9561\n","Epoch 142: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 159ms/step - loss: 0.1226 - accuracy: 0.9561 - val_loss: 0.3915 - val_accuracy: 0.9027\n","Epoch 143/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9565\n","Epoch 143: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1227 - accuracy: 0.9565 - val_loss: 0.3369 - val_accuracy: 0.9073\n","Epoch 144/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9571\n","Epoch 144: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1211 - accuracy: 0.9571 - val_loss: 0.4373 - val_accuracy: 0.8881\n","Epoch 145/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9559\n","Epoch 145: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1252 - accuracy: 0.9559 - val_loss: 0.3524 - val_accuracy: 0.9010\n","Epoch 146/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9588\n","Epoch 146: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 157ms/step - loss: 0.1164 - accuracy: 0.9588 - val_loss: 0.4271 - val_accuracy: 0.8879\n","Epoch 147/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9588\n","Epoch 147: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1179 - accuracy: 0.9588 - val_loss: 0.3930 - val_accuracy: 0.8996\n","Epoch 148/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9585\n","Epoch 148: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1184 - accuracy: 0.9585 - val_loss: 0.3941 - val_accuracy: 0.8958\n","Epoch 149/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1186 - accuracy: 0.9575\n","Epoch 149: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1186 - accuracy: 0.9575 - val_loss: 0.4107 - val_accuracy: 0.8941\n","Epoch 150/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9572\n","Epoch 150: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 63s 161ms/step - loss: 0.1205 - accuracy: 0.9572 - val_loss: 0.3509 - val_accuracy: 0.9089\n","Epoch 151/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9559\n","Epoch 151: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 63s 162ms/step - loss: 0.1227 - accuracy: 0.9559 - val_loss: 0.3768 - val_accuracy: 0.8979\n","Epoch 152/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9591\n","Epoch 152: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1181 - accuracy: 0.9591 - val_loss: 0.3443 - val_accuracy: 0.9067\n","Epoch 153/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9600\n","Epoch 153: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1134 - accuracy: 0.9600 - val_loss: 0.3847 - val_accuracy: 0.9003\n","Epoch 154/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9595\n","Epoch 154: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 159ms/step - loss: 0.1136 - accuracy: 0.9595 - val_loss: 0.3574 - val_accuracy: 0.9076\n","Epoch 155/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9602\n","Epoch 155: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1128 - accuracy: 0.9602 - val_loss: 0.4354 - val_accuracy: 0.8907\n","Epoch 156/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9604\n","Epoch 156: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1121 - accuracy: 0.9604 - val_loss: 0.3891 - val_accuracy: 0.8959\n","Epoch 157/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9603\n","Epoch 157: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 63s 160ms/step - loss: 0.1116 - accuracy: 0.9603 - val_loss: 0.3818 - val_accuracy: 0.9065\n","Epoch 158/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.9587\n","Epoch 158: val_accuracy did not improve from 0.90900\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1151 - accuracy: 0.9587 - val_loss: 0.4320 - val_accuracy: 0.8898\n","Epoch 159/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9609\n","Epoch 159: val_accuracy improved from 0.90900 to 0.91160, saving model to model_save/weights-159-0.9116.hdf5\n","390/390 [==============================] - 63s 161ms/step - loss: 0.1089 - accuracy: 0.9609 - val_loss: 0.3295 - val_accuracy: 0.9116\n","Epoch 160/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9601\n","Epoch 160: val_accuracy did not improve from 0.91160\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1118 - accuracy: 0.9601 - val_loss: 0.3714 - val_accuracy: 0.9047\n","Epoch 161/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9603\n","Epoch 161: val_accuracy did not improve from 0.91160\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1100 - accuracy: 0.9603 - val_loss: 0.3967 - val_accuracy: 0.9000\n","Epoch 162/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9610\n","Epoch 162: val_accuracy did not improve from 0.91160\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1091 - accuracy: 0.9610 - val_loss: 0.4449 - val_accuracy: 0.8926\n","Epoch 163/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9611\n","Epoch 163: val_accuracy did not improve from 0.91160\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1092 - accuracy: 0.9611 - val_loss: 0.3320 - val_accuracy: 0.9085\n","Epoch 164/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9620\n","Epoch 164: val_accuracy did not improve from 0.91160\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1089 - accuracy: 0.9620 - val_loss: 0.3721 - val_accuracy: 0.9052\n","Epoch 165/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9619\n","Epoch 165: val_accuracy did not improve from 0.91160\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1096 - accuracy: 0.9619 - val_loss: 0.3594 - val_accuracy: 0.9085\n","Epoch 166/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9622\n","Epoch 166: val_accuracy did not improve from 0.91160\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1086 - accuracy: 0.9622 - val_loss: 0.3644 - val_accuracy: 0.9057\n","Epoch 167/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9630\n","Epoch 167: val_accuracy did not improve from 0.91160\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1035 - accuracy: 0.9630 - val_loss: 0.3767 - val_accuracy: 0.9015\n","Epoch 168/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9613\n","Epoch 168: val_accuracy improved from 0.91160 to 0.91320, saving model to model_save/weights-168-0.9132.hdf5\n","390/390 [==============================] - 62s 159ms/step - loss: 0.1095 - accuracy: 0.9613 - val_loss: 0.3164 - val_accuracy: 0.9132\n","Epoch 169/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9630\n","Epoch 169: val_accuracy did not improve from 0.91320\n","390/390 [==============================] - 62s 157ms/step - loss: 0.1045 - accuracy: 0.9630 - val_loss: 0.3426 - val_accuracy: 0.9079\n","Epoch 170/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9621\n","Epoch 170: val_accuracy did not improve from 0.91320\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1049 - accuracy: 0.9621 - val_loss: 0.4522 - val_accuracy: 0.8917\n","Epoch 171/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9632\n","Epoch 171: val_accuracy did not improve from 0.91320\n","390/390 [==============================] - 61s 157ms/step - loss: 0.1023 - accuracy: 0.9632 - val_loss: 0.3981 - val_accuracy: 0.9035\n","Epoch 172/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9639\n","Epoch 172: val_accuracy did not improve from 0.91320\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1023 - accuracy: 0.9639 - val_loss: 0.3892 - val_accuracy: 0.9021\n","Epoch 173/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9628\n","Epoch 173: val_accuracy did not improve from 0.91320\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1055 - accuracy: 0.9628 - val_loss: 0.3725 - val_accuracy: 0.9033\n","Epoch 174/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9638\n","Epoch 174: val_accuracy did not improve from 0.91320\n","390/390 [==============================] - 62s 159ms/step - loss: 0.1016 - accuracy: 0.9638 - val_loss: 0.3982 - val_accuracy: 0.9011\n","Epoch 175/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9647\n","Epoch 175: val_accuracy did not improve from 0.91320\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1005 - accuracy: 0.9647 - val_loss: 0.3531 - val_accuracy: 0.9082\n","Epoch 176/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1007 - accuracy: 0.9649\n","Epoch 176: val_accuracy improved from 0.91320 to 0.91380, saving model to model_save/weights-176-0.9138.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.1007 - accuracy: 0.9649 - val_loss: 0.3250 - val_accuracy: 0.9138\n","Epoch 177/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9635\n","Epoch 177: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 64s 163ms/step - loss: 0.1050 - accuracy: 0.9635 - val_loss: 0.5159 - val_accuracy: 0.8819\n","Epoch 178/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9638\n","Epoch 178: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 61s 156ms/step - loss: 0.1001 - accuracy: 0.9638 - val_loss: 0.3699 - val_accuracy: 0.9041\n","Epoch 179/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9653\n","Epoch 179: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 63s 161ms/step - loss: 0.0989 - accuracy: 0.9653 - val_loss: 0.3999 - val_accuracy: 0.9006\n","Epoch 180/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9650\n","Epoch 180: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 63s 161ms/step - loss: 0.1002 - accuracy: 0.9650 - val_loss: 0.4198 - val_accuracy: 0.8944\n","Epoch 181/300\n","391/390 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9648\n","Epoch 181: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 63s 160ms/step - loss: 0.1003 - accuracy: 0.9648 - val_loss: 0.4313 - val_accuracy: 0.8949\n","Epoch 182/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9648\n","Epoch 182: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 63s 160ms/step - loss: 0.0985 - accuracy: 0.9648 - val_loss: 0.3722 - val_accuracy: 0.9016\n","Epoch 183/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9650\n","Epoch 183: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0990 - accuracy: 0.9650 - val_loss: 0.3756 - val_accuracy: 0.9048\n","Epoch 184/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9655\n","Epoch 184: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0984 - accuracy: 0.9655 - val_loss: 0.3618 - val_accuracy: 0.9089\n","Epoch 185/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9656\n","Epoch 185: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0985 - accuracy: 0.9656 - val_loss: 0.3687 - val_accuracy: 0.9044\n","Epoch 186/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9661\n","Epoch 186: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0974 - accuracy: 0.9661 - val_loss: 0.4881 - val_accuracy: 0.8832\n","Epoch 187/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9649\n","Epoch 187: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0992 - accuracy: 0.9649 - val_loss: 0.4171 - val_accuracy: 0.8955\n","Epoch 188/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9656\n","Epoch 188: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0958 - accuracy: 0.9656 - val_loss: 0.3943 - val_accuracy: 0.8965\n","Epoch 189/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9664\n","Epoch 189: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 64s 163ms/step - loss: 0.0973 - accuracy: 0.9664 - val_loss: 0.3589 - val_accuracy: 0.9083\n","Epoch 190/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9659\n","Epoch 190: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0955 - accuracy: 0.9659 - val_loss: 0.4465 - val_accuracy: 0.8916\n","Epoch 191/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9665\n","Epoch 191: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0941 - accuracy: 0.9665 - val_loss: 0.4331 - val_accuracy: 0.8966\n","Epoch 192/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9687\n","Epoch 192: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0912 - accuracy: 0.9687 - val_loss: 0.3745 - val_accuracy: 0.9103\n","Epoch 193/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9691\n","Epoch 193: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0880 - accuracy: 0.9691 - val_loss: 0.3670 - val_accuracy: 0.9072\n","Epoch 194/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9670\n","Epoch 194: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0912 - accuracy: 0.9670 - val_loss: 0.3455 - val_accuracy: 0.9114\n","Epoch 195/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9679\n","Epoch 195: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 63s 162ms/step - loss: 0.0931 - accuracy: 0.9679 - val_loss: 0.3743 - val_accuracy: 0.9054\n","Epoch 196/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9668\n","Epoch 196: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 63s 161ms/step - loss: 0.0912 - accuracy: 0.9668 - val_loss: 0.3481 - val_accuracy: 0.9098\n","Epoch 197/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9664\n","Epoch 197: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 63s 160ms/step - loss: 0.0955 - accuracy: 0.9664 - val_loss: 0.3918 - val_accuracy: 0.9049\n","Epoch 198/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9662\n","Epoch 198: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 63s 161ms/step - loss: 0.0944 - accuracy: 0.9662 - val_loss: 0.4155 - val_accuracy: 0.8992\n","Epoch 199/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9676\n","Epoch 199: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0914 - accuracy: 0.9676 - val_loss: 0.4292 - val_accuracy: 0.8998\n","Epoch 200/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9683\n","Epoch 200: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0890 - accuracy: 0.9683 - val_loss: 0.3544 - val_accuracy: 0.9130\n","Epoch 201/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9686\n","Epoch 201: val_accuracy did not improve from 0.91380\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0907 - accuracy: 0.9686 - val_loss: 0.3641 - val_accuracy: 0.9075\n","Epoch 202/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9690\n","Epoch 202: val_accuracy improved from 0.91380 to 0.91670, saving model to model_save/weights-202-0.9167.hdf5\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0883 - accuracy: 0.9690 - val_loss: 0.3325 - val_accuracy: 0.9167\n","Epoch 203/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9689\n","Epoch 203: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0853 - accuracy: 0.9689 - val_loss: 0.3404 - val_accuracy: 0.9146\n","Epoch 204/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9685\n","Epoch 204: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0911 - accuracy: 0.9685 - val_loss: 0.3752 - val_accuracy: 0.9118\n","Epoch 205/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9691\n","Epoch 205: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 160ms/step - loss: 0.0881 - accuracy: 0.9691 - val_loss: 0.3725 - val_accuracy: 0.9062\n","Epoch 206/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9688\n","Epoch 206: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0878 - accuracy: 0.9688 - val_loss: 0.4007 - val_accuracy: 0.9034\n","Epoch 207/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9687\n","Epoch 207: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0883 - accuracy: 0.9687 - val_loss: 0.4168 - val_accuracy: 0.9009\n","Epoch 208/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9696\n","Epoch 208: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 63s 162ms/step - loss: 0.0869 - accuracy: 0.9696 - val_loss: 0.4031 - val_accuracy: 0.9044\n","Epoch 209/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9702\n","Epoch 209: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 63s 161ms/step - loss: 0.0852 - accuracy: 0.9702 - val_loss: 0.3825 - val_accuracy: 0.9065\n","Epoch 210/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9704\n","Epoch 210: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0851 - accuracy: 0.9704 - val_loss: 0.3840 - val_accuracy: 0.9045\n","Epoch 211/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9698\n","Epoch 211: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0849 - accuracy: 0.9698 - val_loss: 0.3505 - val_accuracy: 0.9090\n","Epoch 212/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9690\n","Epoch 212: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0861 - accuracy: 0.9690 - val_loss: 0.3419 - val_accuracy: 0.9143\n","Epoch 213/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9694\n","Epoch 213: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0870 - accuracy: 0.9694 - val_loss: 0.3745 - val_accuracy: 0.9021\n","Epoch 214/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9696\n","Epoch 214: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0850 - accuracy: 0.9696 - val_loss: 0.3624 - val_accuracy: 0.9100\n","Epoch 215/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9695\n","Epoch 215: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 63s 161ms/step - loss: 0.0832 - accuracy: 0.9695 - val_loss: 0.3420 - val_accuracy: 0.9156\n","Epoch 216/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9706\n","Epoch 216: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0834 - accuracy: 0.9706 - val_loss: 0.3806 - val_accuracy: 0.9099\n","Epoch 217/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9700\n","Epoch 217: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0849 - accuracy: 0.9700 - val_loss: 0.3550 - val_accuracy: 0.9120\n","Epoch 218/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9700\n","Epoch 218: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0855 - accuracy: 0.9700 - val_loss: 0.3616 - val_accuracy: 0.9091\n","Epoch 219/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9705\n","Epoch 219: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 63s 162ms/step - loss: 0.0816 - accuracy: 0.9705 - val_loss: 0.3673 - val_accuracy: 0.9118\n","Epoch 220/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9698\n","Epoch 220: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0845 - accuracy: 0.9698 - val_loss: 0.3477 - val_accuracy: 0.9116\n","Epoch 221/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9710\n","Epoch 221: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0821 - accuracy: 0.9710 - val_loss: 0.4115 - val_accuracy: 0.9038\n","Epoch 222/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9708\n","Epoch 222: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0830 - accuracy: 0.9708 - val_loss: 0.4210 - val_accuracy: 0.9035\n","Epoch 223/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9710\n","Epoch 223: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0834 - accuracy: 0.9710 - val_loss: 0.4574 - val_accuracy: 0.8975\n","Epoch 224/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9715\n","Epoch 224: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0821 - accuracy: 0.9715 - val_loss: 0.3805 - val_accuracy: 0.9081\n","Epoch 225/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9714\n","Epoch 225: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0804 - accuracy: 0.9714 - val_loss: 0.3714 - val_accuracy: 0.9109\n","Epoch 226/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9712\n","Epoch 226: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0822 - accuracy: 0.9712 - val_loss: 0.3545 - val_accuracy: 0.9131\n","Epoch 227/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9726\n","Epoch 227: val_accuracy did not improve from 0.91670\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0799 - accuracy: 0.9726 - val_loss: 0.4329 - val_accuracy: 0.9011\n","Epoch 228/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9723\n","Epoch 228: val_accuracy improved from 0.91670 to 0.92000, saving model to model_save/weights-228-0.9200.hdf5\n","390/390 [==============================] - 63s 161ms/step - loss: 0.0803 - accuracy: 0.9723 - val_loss: 0.3077 - val_accuracy: 0.9200\n","Epoch 229/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9727\n","Epoch 229: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0787 - accuracy: 0.9727 - val_loss: 0.3498 - val_accuracy: 0.9135\n","Epoch 230/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9724\n","Epoch 230: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.3913 - val_accuracy: 0.9052\n","Epoch 231/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9718\n","Epoch 231: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0788 - accuracy: 0.9718 - val_loss: 0.3845 - val_accuracy: 0.9075\n","Epoch 232/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9711\n","Epoch 232: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0807 - accuracy: 0.9711 - val_loss: 0.3752 - val_accuracy: 0.9087\n","Epoch 233/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9714\n","Epoch 233: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0803 - accuracy: 0.9714 - val_loss: 0.3616 - val_accuracy: 0.9119\n","Epoch 234/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9716\n","Epoch 234: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0802 - accuracy: 0.9716 - val_loss: 0.3800 - val_accuracy: 0.9098\n","Epoch 235/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9726\n","Epoch 235: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0779 - accuracy: 0.9726 - val_loss: 0.4827 - val_accuracy: 0.8940\n","Epoch 236/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9724\n","Epoch 236: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0773 - accuracy: 0.9724 - val_loss: 0.4219 - val_accuracy: 0.9005\n","Epoch 237/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9733\n","Epoch 237: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0753 - accuracy: 0.9733 - val_loss: 0.4193 - val_accuracy: 0.9011\n","Epoch 238/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9735\n","Epoch 238: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0751 - accuracy: 0.9735 - val_loss: 0.4125 - val_accuracy: 0.9042\n","Epoch 239/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9737\n","Epoch 239: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0754 - accuracy: 0.9737 - val_loss: 0.3743 - val_accuracy: 0.9115\n","Epoch 240/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9731\n","Epoch 240: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0760 - accuracy: 0.9731 - val_loss: 0.3803 - val_accuracy: 0.9075\n","Epoch 241/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9743\n","Epoch 241: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0737 - accuracy: 0.9743 - val_loss: 0.3875 - val_accuracy: 0.9083\n","Epoch 242/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9755\n","Epoch 242: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0713 - accuracy: 0.9755 - val_loss: 0.3811 - val_accuracy: 0.9097\n","Epoch 243/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9742\n","Epoch 243: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0728 - accuracy: 0.9742 - val_loss: 0.4505 - val_accuracy: 0.8974\n","Epoch 244/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9732\n","Epoch 244: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0743 - accuracy: 0.9732 - val_loss: 0.3799 - val_accuracy: 0.9105\n","Epoch 245/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9731\n","Epoch 245: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0752 - accuracy: 0.9731 - val_loss: 0.3988 - val_accuracy: 0.9059\n","Epoch 246/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9746\n","Epoch 246: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0720 - accuracy: 0.9746 - val_loss: 0.3319 - val_accuracy: 0.9171\n","Epoch 247/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9744\n","Epoch 247: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0749 - accuracy: 0.9744 - val_loss: 0.3671 - val_accuracy: 0.9121\n","Epoch 248/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9732\n","Epoch 248: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0740 - accuracy: 0.9732 - val_loss: 0.4326 - val_accuracy: 0.9042\n","Epoch 249/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9747\n","Epoch 249: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 64s 163ms/step - loss: 0.0722 - accuracy: 0.9747 - val_loss: 0.3991 - val_accuracy: 0.9058\n","Epoch 250/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9759\n","Epoch 250: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0688 - accuracy: 0.9759 - val_loss: 0.3594 - val_accuracy: 0.9124\n","Epoch 251/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9739\n","Epoch 251: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0740 - accuracy: 0.9739 - val_loss: 0.4036 - val_accuracy: 0.9102\n","Epoch 252/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9758\n","Epoch 252: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0709 - accuracy: 0.9758 - val_loss: 0.3432 - val_accuracy: 0.9168\n","Epoch 253/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9732\n","Epoch 253: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0760 - accuracy: 0.9732 - val_loss: 0.3878 - val_accuracy: 0.9046\n","Epoch 254/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9739\n","Epoch 254: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0732 - accuracy: 0.9739 - val_loss: 0.3894 - val_accuracy: 0.9086\n","Epoch 255/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9736\n","Epoch 255: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0730 - accuracy: 0.9736 - val_loss: 0.3471 - val_accuracy: 0.9149\n","Epoch 256/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9741\n","Epoch 256: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0743 - accuracy: 0.9741 - val_loss: 0.3730 - val_accuracy: 0.9097\n","Epoch 257/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9732\n","Epoch 257: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 63s 161ms/step - loss: 0.0754 - accuracy: 0.9732 - val_loss: 0.3967 - val_accuracy: 0.9080\n","Epoch 258/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9749\n","Epoch 258: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0713 - accuracy: 0.9749 - val_loss: 0.3680 - val_accuracy: 0.9131\n","Epoch 259/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9753\n","Epoch 259: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0692 - accuracy: 0.9753 - val_loss: 0.3450 - val_accuracy: 0.9178\n","Epoch 260/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9764\n","Epoch 260: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0685 - accuracy: 0.9764 - val_loss: 0.4124 - val_accuracy: 0.9032\n","Epoch 261/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9759\n","Epoch 261: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0678 - accuracy: 0.9759 - val_loss: 0.3940 - val_accuracy: 0.9074\n","Epoch 262/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9748\n","Epoch 262: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0720 - accuracy: 0.9748 - val_loss: 0.3524 - val_accuracy: 0.9166\n","Epoch 263/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9755\n","Epoch 263: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0688 - accuracy: 0.9755 - val_loss: 0.4170 - val_accuracy: 0.9018\n","Epoch 264/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9755\n","Epoch 264: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0699 - accuracy: 0.9755 - val_loss: 0.3560 - val_accuracy: 0.9131\n","Epoch 265/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9756\n","Epoch 265: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0701 - accuracy: 0.9756 - val_loss: 0.4010 - val_accuracy: 0.9054\n","Epoch 266/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9755\n","Epoch 266: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0668 - accuracy: 0.9755 - val_loss: 0.3554 - val_accuracy: 0.9128\n","Epoch 267/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9775\n","Epoch 267: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0663 - accuracy: 0.9775 - val_loss: 0.3602 - val_accuracy: 0.9102\n","Epoch 268/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9761\n","Epoch 268: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0689 - accuracy: 0.9761 - val_loss: 0.3522 - val_accuracy: 0.9158\n","Epoch 269/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9761\n","Epoch 269: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0683 - accuracy: 0.9761 - val_loss: 0.3697 - val_accuracy: 0.9108\n","Epoch 270/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9767\n","Epoch 270: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 63s 161ms/step - loss: 0.0668 - accuracy: 0.9767 - val_loss: 0.3352 - val_accuracy: 0.9156\n","Epoch 271/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9765\n","Epoch 271: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 63s 162ms/step - loss: 0.0666 - accuracy: 0.9765 - val_loss: 0.3908 - val_accuracy: 0.9106\n","Epoch 272/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9765\n","Epoch 272: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0667 - accuracy: 0.9765 - val_loss: 0.4209 - val_accuracy: 0.9084\n","Epoch 273/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9756\n","Epoch 273: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0688 - accuracy: 0.9756 - val_loss: 0.3631 - val_accuracy: 0.9143\n","Epoch 274/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9766\n","Epoch 274: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0665 - accuracy: 0.9766 - val_loss: 0.3665 - val_accuracy: 0.9142\n","Epoch 275/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9751\n","Epoch 275: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0706 - accuracy: 0.9751 - val_loss: 0.3897 - val_accuracy: 0.9103\n","Epoch 276/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9766\n","Epoch 276: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 63s 162ms/step - loss: 0.0655 - accuracy: 0.9766 - val_loss: 0.3861 - val_accuracy: 0.9136\n","Epoch 277/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9762\n","Epoch 277: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0669 - accuracy: 0.9762 - val_loss: 0.3630 - val_accuracy: 0.9170\n","Epoch 278/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9775\n","Epoch 278: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0637 - accuracy: 0.9775 - val_loss: 0.4808 - val_accuracy: 0.8962\n","Epoch 279/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9760\n","Epoch 279: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0667 - accuracy: 0.9760 - val_loss: 0.3355 - val_accuracy: 0.9196\n","Epoch 280/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9769\n","Epoch 280: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 159ms/step - loss: 0.0644 - accuracy: 0.9769 - val_loss: 0.3815 - val_accuracy: 0.9112\n","Epoch 281/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9776\n","Epoch 281: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0649 - accuracy: 0.9776 - val_loss: 0.3593 - val_accuracy: 0.9148\n","Epoch 282/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9775\n","Epoch 282: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0641 - accuracy: 0.9775 - val_loss: 0.4018 - val_accuracy: 0.9100\n","Epoch 283/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9786\n","Epoch 283: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0606 - accuracy: 0.9786 - val_loss: 0.3551 - val_accuracy: 0.9175\n","Epoch 284/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9757\n","Epoch 284: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0674 - accuracy: 0.9757 - val_loss: 0.3788 - val_accuracy: 0.9147\n","Epoch 285/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9765\n","Epoch 285: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0671 - accuracy: 0.9765 - val_loss: 0.3807 - val_accuracy: 0.9106\n","Epoch 286/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9767\n","Epoch 286: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0655 - accuracy: 0.9767 - val_loss: 0.3834 - val_accuracy: 0.9135\n","Epoch 287/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9779\n","Epoch 287: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0632 - accuracy: 0.9779 - val_loss: 0.3640 - val_accuracy: 0.9149\n","Epoch 288/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9771\n","Epoch 288: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0651 - accuracy: 0.9771 - val_loss: 0.3466 - val_accuracy: 0.9181\n","Epoch 289/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9767\n","Epoch 289: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0654 - accuracy: 0.9767 - val_loss: 0.3587 - val_accuracy: 0.9139\n","Epoch 290/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9772\n","Epoch 290: val_accuracy did not improve from 0.92000\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0646 - accuracy: 0.9772 - val_loss: 0.3718 - val_accuracy: 0.9152\n","Epoch 291/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9773\n","Epoch 291: val_accuracy improved from 0.92000 to 0.92010, saving model to model_save/weights-291-0.9201.hdf5\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0640 - accuracy: 0.9773 - val_loss: 0.3336 - val_accuracy: 0.9201\n","Epoch 292/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9779\n","Epoch 292: val_accuracy did not improve from 0.92010\n","390/390 [==============================] - 62s 157ms/step - loss: 0.0642 - accuracy: 0.9779 - val_loss: 0.3402 - val_accuracy: 0.9195\n","Epoch 293/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9795\n","Epoch 293: val_accuracy did not improve from 0.92010\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0599 - accuracy: 0.9795 - val_loss: 0.3651 - val_accuracy: 0.9149\n","Epoch 294/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9771\n","Epoch 294: val_accuracy did not improve from 0.92010\n","390/390 [==============================] - 63s 161ms/step - loss: 0.0648 - accuracy: 0.9771 - val_loss: 0.4665 - val_accuracy: 0.9001\n","Epoch 295/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9791\n","Epoch 295: val_accuracy did not improve from 0.92010\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0605 - accuracy: 0.9791 - val_loss: 0.3920 - val_accuracy: 0.9135\n","Epoch 296/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9780\n","Epoch 296: val_accuracy did not improve from 0.92010\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0624 - accuracy: 0.9780 - val_loss: 0.4048 - val_accuracy: 0.9064\n","Epoch 297/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9776\n","Epoch 297: val_accuracy did not improve from 0.92010\n","390/390 [==============================] - 61s 157ms/step - loss: 0.0640 - accuracy: 0.9776 - val_loss: 0.3651 - val_accuracy: 0.9167\n","Epoch 298/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9791\n","Epoch 298: val_accuracy did not improve from 0.92010\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0601 - accuracy: 0.9791 - val_loss: 0.4060 - val_accuracy: 0.9086\n","Epoch 299/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9780\n","Epoch 299: val_accuracy did not improve from 0.92010\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0631 - accuracy: 0.9780 - val_loss: 0.3828 - val_accuracy: 0.9125\n","Epoch 300/300\n","391/390 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9783\n","Epoch 300: val_accuracy did not improve from 0.92010\n","390/390 [==============================] - 62s 158ms/step - loss: 0.0619 - accuracy: 0.9783 - val_loss: 0.3417 - val_accuracy: 0.9199\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fd2609a8df0>"]},"metadata":{},"execution_count":18}]}]}